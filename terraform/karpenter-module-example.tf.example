# Example: Using the Karpenter Module with EKS
# This file shows how to integrate the Karpenter module into your main Terraform configuration
#
# Copy this to your main.tf or create a separate karpenter.tf file

# Ensure you have the required providers configured
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.70"
    }
    helm = {
      source  = "hashicorp/helm"
      version = "~> 2.12"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "~> 2.25"
    }
  }
}

# Data sources for existing EKS cluster
data "aws_eks_cluster" "main" {
  name = var.cluster_name
}

data "aws_eks_cluster_auth" "main" {
  name = var.cluster_name
}

# Configure Helm provider
provider "helm" {
  kubernetes {
    host                   = data.aws_eks_cluster.main.endpoint
    cluster_ca_certificate = base64decode(data.aws_eks_cluster.main.certificate_authority[0].data)
    token                  = data.aws_eks_cluster_auth.main.token
  }
}

# Configure Kubernetes provider
provider "kubernetes" {
  host                   = data.aws_eks_cluster.main.endpoint
  cluster_ca_certificate = base64decode(data.aws_eks_cluster.main.certificate_authority[0].data)
  token                  = data.aws_eks_cluster_auth.main.token
}

# AWS provider for ECR Public (us-east-1 only)
provider "aws" {
  alias  = "virginia"
  region = "us-east-1"
}

# Deploy Karpenter using the module
module "karpenter_helm" {
  source = "./modules/karpenter"

  # EKS cluster information
  cluster_name     = module.eks.cluster_name
  cluster_endpoint = module.eks.cluster_endpoint

  # Karpenter version
  karpenter_version = "1.1.1"

  # IAM roles and queue (from EKS Karpenter submodule outputs)
  karpenter_controller_role_arn       = module.eks.karpenter_controller_role_arn
  karpenter_interruption_queue_name   = module.eks.karpenter_queue_name
  karpenter_node_iam_role_name        = module.eks.karpenter_node_iam_role_name

  # High availability configuration
  controller_replicas = 2
  pdb_min_available   = 1

  # Resource allocation
  controller_resources = {
    requests = {
      cpu    = "500m"
      memory = "512Mi"
    }
    limits = {
      cpu    = "1000m"
      memory = "1Gi"
    }
  }

  # Deployment configuration
  namespace          = "kube-system"
  create_namespace   = false
  log_level          = "info"
  enable_webhook     = true

  # Node placement
  node_selector = {
    "karpenter.sh/controller" = "true"
  }

  tolerations = [
    {
      key      = "CriticalAddonsOnly"
      operator = "Exists"
      effect   = "NoSchedule"
      value    = null
    }
  ]

  # Tags
  tags = {
    Environment = "production"
    ManagedBy   = "terraform"
    Component   = "karpenter"
  }

  depends_on = [module.eks]
}

# Optional: Render and apply NodePool templates automatically
locals {
  nodepool_template_vars = {
    node_role_name   = module.eks.karpenter_node_iam_role_name
    cluster_name     = var.cluster_name
    region           = var.region
    cluster_endpoint = module.eks.cluster_endpoint
    vpc_id           = module.vpc.vpc_id
  }
}

# Render x86 NodePool template
resource "local_file" "x86_nodepool" {
  content = templatefile(
    "${path.module}/kubernetes/karpenter/templates/x86-nodepool.yaml.tpl",
    local.nodepool_template_vars
  )
  filename = "${path.module}/kubernetes/karpenter/x86-nodepool-rendered.yaml"
}

# Render ARM64 NodePool template
resource "local_file" "arm64_nodepool" {
  content = templatefile(
    "${path.module}/kubernetes/karpenter/templates/arm64-nodepool.yaml.tpl",
    local.nodepool_template_vars
  )
  filename = "${path.module}/kubernetes/karpenter/arm64-nodepool-rendered.yaml"
}

# Optional: Apply NodePools automatically (requires kubectl configured)
resource "null_resource" "apply_nodepools" {
  depends_on = [
    module.karpenter_helm,
    local_file.x86_nodepool,
    local_file.arm64_nodepool
  ]

  # Trigger re-apply if templates change
  triggers = {
    x86_template    = local_file.x86_nodepool.content
    arm64_template  = local_file.arm64_nodepool.content
  }

  provisioner "local-exec" {
    command = <<-EOT
      # Wait for Karpenter to be ready
      kubectl wait --for=condition=available --timeout=300s \
        deployment/karpenter -n kube-system

      # Apply NodePools
      kubectl apply -f ${local_file.x86_nodepool.filename}
      kubectl apply -f ${local_file.arm64_nodepool.filename}

      # Verify
      kubectl get nodepool
      kubectl get ec2nodeclass
    EOT

    environment = {
      KUBECONFIG = "~/.kube/config"
    }
  }
}

# Outputs
output "karpenter_release_name" {
  description = "Karpenter Helm release name"
  value       = module.karpenter_helm.release_name
}

output "karpenter_release_version" {
  description = "Installed Karpenter version"
  value       = module.karpenter_helm.release_version
}

output "karpenter_status" {
  description = "Karpenter deployment status"
  value       = module.karpenter_helm.status
}

output "rendered_nodepool_files" {
  description = "Paths to rendered NodePool manifests"
  value = {
    x86   = local_file.x86_nodepool.filename
    arm64 = local_file.arm64_nodepool.filename
  }
}
