# OpenTelemetry Collector - Gateway Mode (Centralized Collection)
# Production configuration for gaming platform

gateway:
  enabled: true

  nameOverride: otel-gateway
  fullnameOverride: otel-collector-gateway

  mode: deployment
  replicaCount: 3

  # Pod disruption budget for HA
  podDisruptionBudget:
    enabled: true
    minAvailable: 2

  # Resource allocation for high-throughput processing
  resources:
    limits:
      cpu: 2000m
      memory: 4Gi
    requests:
      cpu: 2000m
      memory: 4Gi

  # Horizontal Pod Autoscaling based on queue size
  autoscaling:
    enabled: true
    minReplicas: 3
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80

  # Pod affinity for distribution across nodes
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                    - otel-collector-gateway
            topologyKey: kubernetes.io/hostname

  # Service configuration
  service:
    type: ClusterIP
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "8888"

  ports:
    # OTLP gRPC receiver
    otlp:
      enabled: true
      containerPort: 4317
      servicePort: 4317
      hostPort: 4317
      protocol: TCP
      appProtocol: grpc
    # OTLP HTTP receiver
    otlp-http:
      enabled: true
      containerPort: 4318
      servicePort: 4318
      protocol: TCP
    # Jaeger gRPC
    jaeger-grpc:
      enabled: true
      containerPort: 14250
      servicePort: 14250
      protocol: TCP
    # Jaeger thrift HTTP
    jaeger-thrift:
      enabled: true
      containerPort: 14268
      servicePort: 14268
      protocol: TCP
    # Zipkin
    zipkin:
      enabled: true
      containerPort: 9411
      servicePort: 9411
      protocol: TCP
    # Prometheus metrics exposure
    metrics:
      enabled: true
      containerPort: 8888
      servicePort: 8888
      protocol: TCP

  # OpenTelemetry Collector Configuration
  config:
    receivers:
      # OTLP receiver for native OTel protocol
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
            max_recv_msg_size_mib: 16
          http:
            endpoint: 0.0.0.0:4318
            cors:
              allowed_origins:
                - "*"

      # Jaeger receiver for legacy applications
      jaeger:
        protocols:
          grpc:
            endpoint: 0.0.0.0:14250
          thrift_http:
            endpoint: 0.0.0.0:14268

      # Zipkin receiver for legacy applications
      zipkin:
        endpoint: 0.0.0.0:9411

      # Prometheus receiver to scrape metrics
      prometheus:
        config:
          scrape_configs:
            # Scrape collector's own metrics
            - job_name: 'otel-collector'
              scrape_interval: 30s
              static_configs:
                - targets: ['localhost:8888']

    processors:
      # Memory limiter to prevent OOM
      memory_limiter:
        check_interval: 1s
        limit_percentage: 80
        spike_limit_percentage: 25

      # Batch processor for efficiency
      batch:
        send_batch_size: 1000
        timeout: 10s
        send_batch_max_size: 1500

      # Resource detection processor
      resource:
        attributes:
          - key: cluster.name
            value: gaming-platform-prod
            action: upsert
          - key: environment
            value: production
            action: upsert
          - key: deployment.type
            value: gateway
            action: upsert

      # Resource detection from environment
      resourcedetection:
        detectors: [env, system, docker, gcp, eks, ec2]
        timeout: 5s

      # K8s attributes processor (requires RBAC)
      k8sattributes:
        auth_type: "serviceAccount"
        passthrough: false
        extract:
          metadata:
            - k8s.namespace.name
            - k8s.deployment.name
            - k8s.statefulset.name
            - k8s.daemonset.name
            - k8s.cronjob.name
            - k8s.job.name
            - k8s.node.name
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.pod.start_time
          labels:
            - tag_name: app.name
              key: app.kubernetes.io/name
              from: pod
            - tag_name: app.version
              key: app.kubernetes.io/version
              from: pod
            - tag_name: app.component
              key: app.kubernetes.io/component
              from: pod

      # Tail sampling for traces - keep interesting traces
      tail_sampling:
        policies:
          # Always sample errors
          - name: error-sample
            type: status_code
            status_code:
              status_codes: [ERROR]
          # Always sample slow requests (> 2s)
          - name: slow-traces
            type: latency
            latency:
              threshold_ms: 2000
          # Sample 10% of successful requests
          - name: probabilistic-sample
            type: probabilistic
            probabilistic:
              sampling_percentage: 10
          # Always sample specific game events
          - name: game-critical-events
            type: string_attribute
            string_attribute:
              key: event.type
              values:
                - player.death
                - player.levelup
                - purchase.completed
                - matchmaking.started
          # Sample rate limiting to prevent overload
          - name: rate-limiting
            type: rate_limiting
            rate_limiting:
              spans_per_second: 1000
        decision_wait: 10s
        num_traces: 100000
        expected_new_traces_per_sec: 1000

      # Attributes processor to add/modify attributes
      attributes:
        actions:
          # Add platform identifier
          - key: platform
            value: gaming
            action: insert
          # Add region
          - key: cloud.region
            value: us-east-1
            action: insert
          # Remove sensitive data
          - key: password
            action: delete
          - key: token
            action: delete
          - key: authorization
            action: delete

      # Filter processor to drop unwanted telemetry
      filter:
        error_mode: ignore
        traces:
          span:
            # Drop health check traces
            - 'attributes["http.target"] == "/health"'
            - 'attributes["http.target"] == "/healthz"'
            - 'attributes["http.target"] == "/readiness"'
        metrics:
          metric:
            # Drop metrics with test labels
            - 'resource.attributes["environment"] == "test"'
        logs:
          log_record:
            # Drop debug logs in production
            - 'severity_number < SEVERITY_NUMBER_INFO'

      # Transform processor for advanced transformations
      transform:
        error_mode: ignore
        trace_statements:
          - context: span
            statements:
              # Normalize HTTP methods
              - set(attributes["http.method"], Uppercase(attributes["http.method"]))
              # Calculate request duration in ms
              - set(attributes["duration_ms"], (end_time_unix_nano - start_time_unix_nano) / 1000000)
        metric_statements:
          - context: metric
            statements:
              # Add metric type
              - set(attributes["metric.type"], "application")
        log_statements:
          - context: log
            statements:
              # Normalize severity
              - set(severity_text, Uppercase(severity_text))

    exporters:
      # Debug exporter for troubleshooting
      debug:
        verbosity: detailed
        sampling_initial: 5
        sampling_thereafter: 200

      # Prometheus Remote Write for metrics
      prometheusremotewrite:
        endpoint: http://prometheus-server.monitoring.svc.cluster.local:9090/api/v1/write
        resource_to_telemetry_conversion:
          enabled: true
        target_info:
          enabled: true
        tls:
          insecure: true
        retry_on_failure:
          enabled: true
          initial_interval: 5s
          max_interval: 30s
          max_elapsed_time: 300s
        sending_queue:
          enabled: true
          num_consumers: 10
          queue_size: 5000

      # Loki exporter for logs
      loki:
        endpoint: http://loki-gateway.monitoring.svc.cluster.local/loki/api/v1/push
        format: json
        labels:
          resource:
            cluster: "cluster.name"
            namespace: "k8s.namespace.name"
            pod: "k8s.pod.name"
            container: "k8s.container.name"
          attributes:
            level: "severity_text"
            app: "app.name"
        tls:
          insecure: true

      # OTLP exporter for Tempo (traces)
      otlp/tempo:
        endpoint: tempo-distributor.monitoring.svc.cluster.local:4317
        tls:
          insecure: true
        compression: gzip
        sending_queue:
          enabled: true
          num_consumers: 10
          queue_size: 5000
        retry_on_failure:
          enabled: true
          initial_interval: 5s
          max_interval: 30s

      # OTLP exporter for Pyroscope (profiling)
      otlp/pyroscope:
        endpoint: pyroscope.monitoring.svc.cluster.local:4317
        tls:
          insecure: true
        compression: gzip

    extensions:
      # Health check extension
      health_check:
        endpoint: 0.0.0.0:13133

      # pprof for performance profiling
      pprof:
        endpoint: 0.0.0.0:1777

      # zpages for live debugging
      zpages:
        endpoint: 0.0.0.0:55679

    service:
      extensions: [health_check, pprof, zpages]

      pipelines:
        # Traces pipeline
        traces:
          receivers: [otlp, jaeger, zipkin]
          processors:
            - memory_limiter
            - resource
            - resourcedetection
            - k8sattributes
            - attributes
            - filter
            - transform
            - tail_sampling
            - batch
          exporters: [otlp/tempo, debug]

        # Metrics pipeline
        metrics:
          receivers: [otlp, prometheus]
          processors:
            - memory_limiter
            - resource
            - resourcedetection
            - k8sattributes
            - attributes
            - filter
            - transform
            - batch
          exporters: [prometheusremotewrite, debug]

        # Logs pipeline
        logs:
          receivers: [otlp]
          processors:
            - memory_limiter
            - resource
            - resourcedetection
            - k8sattributes
            - attributes
            - filter
            - transform
            - batch
          exporters: [loki, debug]

      telemetry:
        logs:
          level: info
        metrics:
          level: detailed
          address: 0.0.0.0:8888

  # Service Account with RBAC permissions
  serviceAccount:
    create: true
    annotations: {}

  # RBAC for k8sattributes processor
  clusterRole:
    create: true
    rules:
      - apiGroups:
          - ""
        resources:
          - pods
          - namespaces
          - nodes
        verbs:
          - get
          - watch
          - list
      - apiGroups:
          - apps
        resources:
          - replicasets
          - deployments
          - daemonsets
          - statefulsets
        verbs:
          - get
          - watch
          - list

  clusterRoleBinding:
    create: true

  # Monitoring
  podMonitor:
    enabled: true
    metricsEndpoints:
      - port: metrics
        interval: 30s

  # Security
  podSecurityContext:
    runAsNonRoot: true
    runAsUser: 10001
    fsGroup: 10001
    seccompProfile:
      type: RuntimeDefault

  securityContext:
    allowPrivilegeEscalation: false
    readOnlyRootFilesystem: true
    capabilities:
      drop:
        - ALL

  # Probes
  livenessProbe:
    httpGet:
      path: /
      port: 13133
    initialDelaySeconds: 30
    periodSeconds: 30
    timeoutSeconds: 5
    failureThreshold: 3

  readinessProbe:
    httpGet:
      path: /
      port: 13133
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3

  # Environment variables
  env:
    - name: GOMEMLIMIT
      value: "3600MiB"
    - name: K8S_NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    - name: K8S_POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    - name: K8S_POD_NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace
    - name: K8S_POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP

# DaemonSet collector - separate deployment
# Enabled via values-daemonset.yaml overlay
daemonset:
  enabled: false

# Network policy
networkPolicy:
  enabled: true
  policyTypes:
    - Ingress
    - Egress
  ingress:
    # Allow from all pods (apps sending telemetry)
    - from:
        - namespaceSelector: {}
      ports:
        - protocol: TCP
          port: 4317
        - protocol: TCP
          port: 4318
        - protocol: TCP
          port: 14250
        - protocol: TCP
          port: 14268
        - protocol: TCP
          port: 9411
  egress:
    # Allow to monitoring backends
    - to:
        - namespaceSelector:
            matchLabels:
              name: monitoring
      ports:
        - protocol: TCP
          port: 9090  # Prometheus
        - protocol: TCP
          port: 3100  # Loki
        - protocol: TCP
          port: 4317  # Tempo/Pyroscope
    # Allow DNS
    - to:
        - namespaceSelector: {}
          podSelector:
            matchLabels:
              k8s-app: kube-dns
      ports:
        - protocol: UDP
          port: 53
    # Allow to Kubernetes API
    - to:
        - namespaceSelector: {}
          podSelector:
            matchLabels:
              component: apiserver
      ports:
        - protocol: TCP
          port: 443
